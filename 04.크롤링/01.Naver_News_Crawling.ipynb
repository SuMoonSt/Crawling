{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "\n",
    "def parse_relative_time(time_str):\n",
    "    \"\"\"Parse relative time string and return absolute datetime.\"\"\"\n",
    "    now = datetime.now()\n",
    "    \n",
    "    if \"분 전\" in time_str:\n",
    "        minutes = int(time_str.replace(\"분 전\", \"\").strip())\n",
    "        return now - timedelta(minutes=minutes)\n",
    "    elif \"시간 전\" in time_str:\n",
    "        hours = int(time_str.replace(\"시간 전\", \"\").strip())\n",
    "        return now - timedelta(hours=hours)\n",
    "    elif \"일 전\" in time_str:\n",
    "        days = int(time_str.replace(\"일 전\", \"\").strip())\n",
    "        return now - timedelta(days=days)\n",
    "    elif \"주 전\" in time_str:\n",
    "        weeks = int(time_str.replace(\"주 전\", \"\").strip())\n",
    "        return now - timedelta(weeks=weeks)\n",
    "    else:\n",
    "        return now  # Default to now if unknown format\n",
    "\n",
    "def extract_data(soup):\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    div = soup.find(\"div\", class_=\"group_news\")\n",
    "    if div:\n",
    "        for subdiv in div.find_all(\"div\", class_=\"news_area\"):\n",
    "            link = title = news_agency = date = \"N/A\"\n",
    "\n",
    "            contents_tag = subdiv.find(\"a\",class_=\"news_tit\")\n",
    "            if contents_tag and \"href\" in contents_tag.attrs and \"title\" in contents_tag.attrs:\n",
    "                link = contents_tag[\"href\"]\n",
    "                #print(\"Link: \",link)\n",
    "                title = contents_tag[\"title\"]\n",
    "                #print(\"Title: \",title)\n",
    "                \n",
    "            info_tag = subdiv.find(\"div\",class_=\"info_group\")\n",
    "            if info_tag:\n",
    "                news_agency_tag = info_tag.find(\"a\",class_ = \"info press\")\n",
    "                news_agency = news_agency_tag.get_text(strip=True) if news_agency_tag else \"N/A\"\n",
    "                #print(\"News Agency: \",news_agency)\n",
    "\n",
    "                date_tag = info_tag.find(\"span\", class_=\"info\")\n",
    "                if date_tag:\n",
    "                    relative_time = date_tag.get_text(strip=True)\n",
    "                    if \"분\" in relative_time or \"시간\" in relative_time:\n",
    "                        date = parse_relative_time(relative_time).strftime('%Y-%m-%d %H:%M:00')\n",
    "                    else:\n",
    "                        date = parse_relative_time(relative_time).strftime('%Y-%m-%d 00:00:00')\n",
    "                else:\n",
    "                    date = \"N/A\"\n",
    "                #print(\"Date: \",date)\n",
    "\n",
    "            result.append([title,news_agency,date,link])\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"No div with class 'list_body newsflash_body' found\")\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_request(keyword, sortidx, start=1):\n",
    "    # 입력된 분야에 맞는 request 객체를 반환\n",
    "    # 아래 url에 쿼리를 적용한 것을 반환\n",
    "    custom_header = {\n",
    "        'referer' : 'https://www.naver.com/',\n",
    "        'user-agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/73.0.3683.103 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    url = f\"https://search.naver.com/search.naver?where=news&sort={sortidx}\"\n",
    "    \n",
    "    try:\n",
    "        req = requests.get(url, headers=custom_header, params={\"query\": keyword, \"start\": start})\n",
    "        req.raise_for_status()\n",
    "        return req\n",
    "    # Raise an error for bad status codes\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Request failed: {e}\")\n",
    "        return None\n",
    "    \n",
    "def save_to_excel(list_act,keyword):\n",
    "    excel_path = \"result.xlsx\"\n",
    "    sheet_name = ''.join(char for char in keyword if char.isalnum())[:31]\n",
    "\n",
    "    with pd.ExcelWriter(excel_path, engine=\"openpyxl\") as writer:\n",
    "        df = pd.DataFrame(list_act, columns=['뉴스제목', '뉴스사', '게시일', '링크'])\n",
    "        df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "            \n",
    "    wb = load_workbook(excel_path)\n",
    "    ws = wb[sheet_name]\n",
    "    for sheet_name in wb.sheetnames:\n",
    "        for col in ws.columns:\n",
    "            max_length = 0\n",
    "            column_letter = col[0].column_letter\n",
    "            for cell in col:\n",
    "                try:\n",
    "                    if len(str(cell.value)) > max_length:\n",
    "                        max_length = len(cell.value)\n",
    "                except:\n",
    "                    pass\n",
    "            adjusted_width = (max_length + 5)  # Add some extra space\n",
    "            ws.column_dimensions[column_letter].width = adjusted_width\n",
    "    wb.save(excel_path)\n",
    "    print(\"Data has been saved to 'result.xlsx' with multiple sheets.\")\n",
    "\n",
    "def main():\n",
    "    all_results = []\n",
    "    keyword = input('키워드를 입력하세요.\\n > ')\n",
    "    max_pages = 3  # 원하는 페이지 수\n",
    "    results_per_page = 10\n",
    "    sortidx = 0 #최신순1\n",
    "\n",
    "    for page in range(1, max_pages + 1):\n",
    "        start = (page - 1) * results_per_page + 1\n",
    "        req = get_request(keyword, sortidx, start=start)\n",
    "        \n",
    "        if req:\n",
    "            soup = BeautifulSoup(req.text, \"html.parser\")\n",
    "            results = extract_data(soup)\n",
    "            all_results.extend(results)\n",
    "        else:\n",
    "            print(\"Failed to retrieve data.\")\n",
    "            break\n",
    "\n",
    "    if all_results:\n",
    "        save_to_excel(all_results, keyword)\n",
    "    else:\n",
    "        print(\"No data to save.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
